
# Robots Disallowed Wordlist
[Robots Disallowed](https://github.com/danielmiessler/RobotsDisallowed) is a project maintained on GitHub as a replacement for the [RAFT tool](https://code.google.com/archive/p/raft/) (which is not maintained). Both projects create(d) wordlists using the `robot.txt` files of indexed websites. This file tells spidering indexers what pages a site does *not* want to be indexed.

> [!Resources]
> - [Robots Disallowed GitHub](https://github.com/danielmiessler/RobotsDisallowed)
> - [RAFT tool](https://code.google.com/archive/p/raft/)